{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPmtfM3xqNIeZGMCgqcODMx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PSX-Ramitas/CSCI-166/blob/main/166_ArmSim.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Pybullet Simulation Software\n",
        "\n",
        "We are using pybullet for our simulation softwar this needs to donloaded to use in collab."
      ],
      "metadata": {
        "id": "kEpFJCbYDBzN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SiPXXUnfnSe_",
        "outputId": "7959c224-9dba-49e9-c867-020aa98efcfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pybullet\n",
            "  Downloading pybullet-3.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Downloading pybullet-3.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (103.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.2/103.2 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pybullet\n",
            "Successfully installed pybullet-3.2.6\n"
          ]
        }
      ],
      "source": [
        "pip install -U pybullet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we do all the imports this includes things from pybullet as well as the pytorch libraries."
      ],
      "metadata": {
        "id": "ffEf4uNWDX4E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple, deque\n",
        "from itertools import count\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import pybullet as p\n",
        "import time\n",
        "import pybullet_data\n",
        "import os\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "UyhpVIMgonsK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we mount google drive to load in stuff for the simulation as well as save our data and models."
      ],
      "metadata": {
        "id": "2LBj9ozbDfDO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNKDzDS-rsmh",
        "outputId": "588bfc30-93dd-447f-960f-c520009c4610"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a class for the simulations\n",
        "\n",
        "This class will handle all the back end for the simulation form simply getting it started to loading up all the assets.  It also handles what to do in the case of collisions and gives the rewards for each session.  The actual simulation situation was designed and implimented by our team with pybullet just being used as a physics engine.\n",
        "\n",
        "We modeled the arm in blender and defined how it could behave in a urdf file.  We also implemented the reward scheme for the robots behavior."
      ],
      "metadata": {
        "id": "AU5hepnFDmnR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ArmEnv:\n",
        "    def __init__(self, gui=True, mSteps=500):\n",
        "        # initialize variables\n",
        "        self.gui = gui\n",
        "        self.physics_client = None\n",
        "        self.planeId = None\n",
        "        self.armId = None\n",
        "        self.cubeId = None\n",
        "        self.Tray = None\n",
        "        self.stepSize = 0.00872665\n",
        "        self.actionSpace = 729\n",
        "        self.cubeStartPos = [0.1, 0.15, 0.05]\n",
        "        self.trayStartPos = [-0.05, 0.2, 0.01]\n",
        "        self.endPoint = self.trayStartPos.copy()\n",
        "        self.endPoint[2] += 0.2\n",
        "\n",
        "        # set up variables for camera\n",
        "        self.cameraYawId = 17\n",
        "        self.cameraPitchId = 18\n",
        "        self.cameraYaw = -0.3\n",
        "        self.cameraPitch = -0.8\n",
        "\n",
        "        # an array of servo Ids\n",
        "        # goes from base upward\n",
        "        # last two are right claw and left claw\n",
        "        self.ServoIds = [2, 3, 4, 7, 9, 13, 15]\n",
        "\n",
        "        # array for servo angles\n",
        "        # corresponds to servo ids\n",
        "        self.ServoAngles = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
        "\n",
        "        # variables to keep track of updates\n",
        "        self.reward = 0\n",
        "        self.phase = 0\n",
        "        self.state = None\n",
        "        self.padTouch = False\n",
        "        self.finished = False\n",
        "        self.CubeTouchTray = False\n",
        "        self.CubeTouchFloor = False\n",
        "        self.steps = 0\n",
        "        self.maxSteps = mSteps\n",
        "\n",
        "    def connect(self):\n",
        "        mode = p.GUI if self.gui else p.DIRECT\n",
        "        self.physics_client = p.connect(mode)\n",
        "        p.setAdditionalSearchPath(pybullet_data.getDataPath())\n",
        "        print(\"connected to pybullet simulation\")\n",
        "\n",
        "    def disconnect(self):\n",
        "        # diconnect form the simulation\n",
        "        if self.physics_client is not None:\n",
        "            p.disconnect()\n",
        "            self.physics_client = None\n",
        "            self.planeId = None\n",
        "            self.armId = None\n",
        "            print(\"diconnected from pybullet simulation\")\n",
        "\n",
        "    def load_environment(self):\n",
        "        # Set the initial conditions of the simulation\n",
        "        p.setGravity(0,0,-9.8)\n",
        "        self.planeId = p.loadURDF(\"plane.urdf\")\n",
        "        startPos = [0,0,0.01]\n",
        "        startOrientation = p.getQuaternionFromEuler([0,0,0])\n",
        "        self.armId = p.loadURDF(\"/content/drive/MyDrive/166_Project/ArmObj/Robot_Arm.urdf\", startPos, startOrientation, useFixedBase=True)\n",
        "        self.cubeId = p.loadURDF(\"/content/drive/MyDrive/166_Project/ArmObj/Cube.urdf\", self.cubeStartPos)\n",
        "        self.TrayId = p.loadURDF(\"/content/drive/MyDrive/166_Project/ArmObj/Tray.urdf\", self.trayStartPos)\n",
        "\n",
        "        # Get the total number of joints (which is also the number of child links)\n",
        "        num_joints = p.getNumJoints(self.armId)\n",
        "\n",
        "\n",
        "        # Enable collisions between all links of the same object\n",
        "        for joint_index in range(num_joints):\n",
        "            # Setting collision filter to allow self-collision (i.e., link with itself)\n",
        "            p.setCollisionFilterGroupMask(self.armId, joint_index, -1, -1)  # Enable self-collision for all links\n",
        "\n",
        "        # Set the joint angle (position control mode)\n",
        "        # move camera yaw to correct position\n",
        "        p.setJointMotorControl2(\n",
        "            self.armId,             # Robot ID\n",
        "            self.cameraYawId,            # Joint index\n",
        "            controlMode=p.POSITION_CONTROL,  # Position control mode\n",
        "            targetPosition=self.cameraYaw,    # Target joint angle in radians\n",
        "            force=500            # Maximum force to apply\n",
        "        )\n",
        "\n",
        "        # move camera pitch to correct position\n",
        "        p.setJointMotorControl2(\n",
        "            self.armId,             # Robot ID\n",
        "            self.cameraPitchId,            # Joint index\n",
        "            controlMode=p.POSITION_CONTROL,  # Position control mode\n",
        "            targetPosition=self.cameraPitch,    # Target joint angle in radians\n",
        "            force=500            # Maximum force to apply\n",
        "        )\n",
        "\n",
        "        for i in range(50):\n",
        "            self.step()\n",
        "\n",
        "    def resetSim(self):\n",
        "        if self.physics_client is not None:\n",
        "            p.resetSimulation()\n",
        "\n",
        "            # array for servo angles\n",
        "            # corresponds to servo ids\n",
        "            self.ServoAngles = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
        "\n",
        "            # variables to keep track of updates\n",
        "            self.reward = 0\n",
        "            self.phase = 0\n",
        "            self.state = None\n",
        "            self.padTouch = False\n",
        "            self.finished = False\n",
        "            self.CubeTouchTray = False\n",
        "            self.CubeTouchFloor = False\n",
        "            self.steps = 0\n",
        "\n",
        "            self.load_environment()\n",
        "\n",
        "            rgb_image = self.getCameraImage()\n",
        "\n",
        "            # Convert the image from RGBA (4 channels) to RGB (3 channels)\n",
        "            rgb_image = rgb_image[:, :, :3]\n",
        "\n",
        "            self.state = [rgb_image, self.ServoAngles]\n",
        "\n",
        "        return self.state\n",
        "\n",
        "    def setMotorsPosition(self):\n",
        "        # set default servo joint states\n",
        "\n",
        "        for i in range(7):\n",
        "            # move camera pitch to correct position\n",
        "            p.setJointMotorControl2(\n",
        "                self.armId,             # Robot ID\n",
        "                self.ServoIds[i],            # Joint index\n",
        "                controlMode=p.POSITION_CONTROL,  # Position control mode\n",
        "                targetPosition=self.ServoAngles[i],    # Target joint angle in radians\n",
        "                force=100,            # Maximum force to apply\n",
        "                positionGain=1,  # Increase stiffness\n",
        "                velocityGain=1   # Increase damping\n",
        "            )\n",
        "\n",
        "    def setMotorVelocity(self):\n",
        "        # set default servo joint states\n",
        "\n",
        "        for i in range(7):\n",
        "            # move camera pitch to correct position\n",
        "            p.setJointMotorControl2(\n",
        "                self.armId,             # Robot ID\n",
        "                self.ServoIds[i],            # Joint index\n",
        "                controlMode=p.VELOCITY_CONTROL,  # Position control mode\n",
        "                targetVelocity=0,\n",
        "                force=10,            # Maximum force to apply\n",
        "            )\n",
        "\n",
        "    def updateServo(self, action: int, servo: int):\n",
        "        # if action is 1 then move by positive tenth of a degree\n",
        "        if (action == 1):\n",
        "            self.ServoAngles[servo] -= self.stepSize\n",
        "            if (self.ServoAngles[servo] <= -1.57):  # check if went beyond limits\n",
        "                self.reward -= 10\n",
        "                return False\n",
        "\n",
        "        # if action is 2 then move by a negative tenth of a degree\n",
        "        elif (action == 2):\n",
        "            self.ServoAngles[servo] += self.stepSize\n",
        "            if (self.ServoAngles[servo] >= 1.57): #check if went beyond limits\n",
        "                self.reward -= 10\n",
        "                return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def updateClawsServo(self, action: int):\n",
        "        # if action is 1 then move by positive tenth of a degree\n",
        "        if (action == 1):\n",
        "            self.ServoAngles[5] += self.stepSize\n",
        "            self.ServoAngles[6] -= self.stepSize\n",
        "            if (self.ServoAngles[5] >= 0):  # check if went beyond limits\n",
        "                self.reward -= 10\n",
        "                return False\n",
        "\n",
        "        # if action is 2 then move by a negative tenth of a degree\n",
        "        elif (action == 2):\n",
        "            self.ServoAngles[5] -= self.stepSize\n",
        "            self.ServoAngles[6] += self.stepSize\n",
        "            if (self.ServoAngles[5] <= -0.785398): #check if went beyond limits\n",
        "                self.reward -= 10\n",
        "                return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def collisionCheck(self):\n",
        "        contact_points = p.getContactPoints()\n",
        "        self.padTouch = False\n",
        "\n",
        "        for contact in contact_points:\n",
        "            # if contact is with the ground\n",
        "            if contact[1] == self.planeId:\n",
        "                # if contact is with robot\n",
        "                if contact[2] == self.armId:\n",
        "                    # check that contact is not with the base\n",
        "                    if contact[4] != -1:\n",
        "                      self.reward -= 10\n",
        "                      return False\n",
        "\n",
        "                # check if contact is with the cube\n",
        "                # also check what phase it is, only matter if in phase 3\n",
        "                # to not let cube drag on floor\n",
        "                elif contact[2] == self.cubeId:\n",
        "                    self.CubeTouchFloor = True\n",
        "\n",
        "            # check if it is the robot contacting something else\n",
        "            # also contact will not be with ground since that would appear first\n",
        "            elif contact[1] == self.armId:\n",
        "                # make sure it is the cube that is being touched\n",
        "                if contact[2] == self.cubeId:\n",
        "                    # if the contact is not with the touch pad, if so return false and have penalty\n",
        "                    if contact[3] != 14 and contact[3] != 15:\n",
        "                        self.reward -= 10\n",
        "                        return False\n",
        "                    # if it is both touch pad touching the cube then set flag to true\n",
        "                    elif contact[3] == 14 and contact[3] == 15:\n",
        "                        self.padTouch = True\n",
        "                else:\n",
        "                    self.reward -= 10\n",
        "                    return False\n",
        "\n",
        "            elif contact[1] == self.cubeId:\n",
        "                if contact[2] == self.TrayId:\n",
        "                    if contact[4] == -1:\n",
        "                        self.CubeTouchTray = True\n",
        "\n",
        "\n",
        "        return True\n",
        "\n",
        "\n",
        "    def takeAction(self, action: int):\n",
        "        self.reward = 0\n",
        "        self.steps += 1\n",
        "\n",
        "        prev_pos = self.ServoAngles.copy()\n",
        "        control = self.convInttoBase3(action)\n",
        "        take_step = True\n",
        "        pos_ClawL = np.array(p.getLinkState(self.armId, 14)[0])\n",
        "        pos_ClawR = np.array(p.getLinkState(self.armId, 15)[0])\n",
        "        prev_cube = p.getBasePositionAndOrientation(self.cubeId)\n",
        "\n",
        "        prev_pos_C = ((pos_ClawL - pos_ClawR) * 0.5) + pos_ClawR\n",
        "\n",
        "        # loop through the servos updating positions\n",
        "        for i in range(5):\n",
        "            take_step = (take_step and self.updateServo(control[i], i))\n",
        "\n",
        "        # update claw seperately since it is two joints in simulation but one servo in real world\n",
        "        take_step = (take_step and self.updateClawsServo(control[5]))\n",
        "\n",
        "        if take_step:\n",
        "            # run simulation for a moment to let arm update itself\n",
        "            # check for collisions then and penalise for them\n",
        "            for i in range(5):\n",
        "                self.step()\n",
        "                take_step = (take_step and self.collisionCheck())\n",
        "                if not take_step:\n",
        "                    break\n",
        "\n",
        "            # if there is a collision don't do this and simply back step\n",
        "\n",
        "            if take_step:\n",
        "                pos1 = np.array(p.getLinkState(self.armId, 14)[0])\n",
        "                pos2 = np.array(p.getLinkState(self.armId, 15)[0])\n",
        "\n",
        "                posC = ((pos1 - pos2) * 0.5) + pos2\n",
        "\n",
        "                cube, _ = p.getBasePositionAndOrientation(self.cubeId)\n",
        "\n",
        "                dist1 = math.dist(cube, prev_pos_C)\n",
        "                dist2 = math.dist(cube, posC)\n",
        "\n",
        "                if self.phase == 0:\n",
        "                    prev_dist = abs(0.3 - prev_pos[6])\n",
        "                    dist = abs(0.3 - self.ServoAngles[6])\n",
        "                    if prev_dist > dist:\n",
        "                        self.reward += 1\n",
        "                    elif prev_dist < dist:\n",
        "                        self.reward -= 1\n",
        "\n",
        "                    if dist1 > dist2:\n",
        "                        self.reward += 2\n",
        "                    elif dist1 < dist2:\n",
        "                        self.reward -= 2\n",
        "\n",
        "                    if dist2 < 0.02:\n",
        "                        self.phase = 1\n",
        "                        self.reward += 20\n",
        "\n",
        "                elif self.phase == 1:\n",
        "                    prev_dist = abs(0.3 - prev_pos[6])\n",
        "                    dist = abs(0.3 - self.ServoAngles[6])\n",
        "                    if prev_dist < dist:\n",
        "                        self.reward += 1\n",
        "                    elif prev_dist > dist:\n",
        "                        self.reward -= 1\n",
        "\n",
        "                    if dist1 > dist2:\n",
        "                        self.reward += 1\n",
        "                    elif dist1 < dist2:\n",
        "                        self.reward -= 1\n",
        "\n",
        "                    if dist2 > 0.03:\n",
        "                        self.phase = 1\n",
        "                        self.reward -= 10\n",
        "\n",
        "                    if self.padTouch:\n",
        "                        self.phase = 2\n",
        "\n",
        "                elif self.phase == 2:\n",
        "\n",
        "                    if dist2 > dist1:\n",
        "                        self.reward -= 1\n",
        "\n",
        "                    if self.CubeTouchFloor:\n",
        "                        self.reward -= 1\n",
        "\n",
        "                    if dist2 > 0.03:\n",
        "                        self.phase = 1\n",
        "                        self.reward -= 10\n",
        "\n",
        "                    dist3 = math.dist(cube, self.endPoint)\n",
        "                    if dist3 < 0.02:\n",
        "                        self.reward += 20\n",
        "                        self.phase = 3\n",
        "\n",
        "                elif self.phase == 3:\n",
        "                    prev_dist = abs(0.3 - prev_pos[6])\n",
        "                    dist = abs(0.3 - self.ServoAngles[6])\n",
        "                    if prev_dist > dist:\n",
        "                        self.reward += 1\n",
        "                    elif prev_dist < dist:\n",
        "                        self.reward -= 1\n",
        "\n",
        "                    if dist2 > 0.03 and self.CubeTouchFloor:\n",
        "                        self.phase = 1\n",
        "                        self.reward -= 10\n",
        "\n",
        "                    tray, _ = p.getBasePositionAndOrientation(self.TrayId)\n",
        "                    dist4 = math.dist(cube, tray)\n",
        "                    dist5 = math.dist(prev_cube, tray)\n",
        "                    if dist4 < dist5:\n",
        "                        self.reward += 1\n",
        "\n",
        "                    if self.CubeTouchTray:\n",
        "                        self.reward += 100\n",
        "                        self.finished = True\n",
        "\n",
        "            else:\n",
        "                self.ServoAngles = prev_pos\n",
        "\n",
        "        else:\n",
        "            self.ServoAngles = prev_pos\n",
        "\n",
        "        self.CubeTouchFloor = False\n",
        "\n",
        "        rgb_image = self.getCameraImage()\n",
        "\n",
        "        # Convert the image from RGBA (4 channels) to RGB (3 channels)\n",
        "        rgb_image = rgb_image[:, :, :3]\n",
        "\n",
        "        observation = [rgb_image, self.ServoAngles]\n",
        "\n",
        "        return observation, self.reward, self.finished\n",
        "\n",
        "\n",
        "    def step(self):\n",
        "        self.setMotorsPosition()\n",
        "        p.stepSimulation()\n",
        "\n",
        "    def getCameraImage(self):\n",
        "        # get position and orientation of the link representing physical camera\n",
        "        camera_position, camera_orientation,_,_,_,_  = p.getLinkState(self.armId, 19)\n",
        "\n",
        "        # set camera offset so it is not in the link\n",
        "        camera_offset = [0, 0.015, 0]\n",
        "\n",
        "        # Calculate the camera's eye position by adding the offset to the robot's position\n",
        "        eye_position = [camera_position[0] + camera_offset[0],\n",
        "                        camera_position[1] + camera_offset[1],\n",
        "                        camera_position[2] + camera_offset[2]]\n",
        "\n",
        "        # increase the offset a bit so it is looking ahead\n",
        "        camera_offset = [0, 1, 0]\n",
        "\n",
        "        camera_orientation = p.getEulerFromQuaternion(camera_orientation)\n",
        "        Roll  = camera_orientation[1] * (180.0 / math.pi)\n",
        "        Pitch = camera_orientation[0] * (180.0 / math.pi)\n",
        "        Yaw   = camera_orientation[2] * (180.0 / math.pi)\n",
        "\n",
        "        # Compute the view matrix (camera's position and orientation)\n",
        "        view_matrix = p.computeViewMatrixFromYawPitchRoll(eye_position,\n",
        "                                                          distance=0.1,\n",
        "                                                          yaw=Yaw,\n",
        "                                                          pitch=Pitch,\n",
        "                                                          roll=Roll,\n",
        "                                                          upAxisIndex=2)\n",
        "\n",
        "        # Define the projection matrix (field of view, aspect ratio, near/far planes)\n",
        "        fov = 60  # Field of view in degrees\n",
        "        aspect_ratio = 1.0  # Aspect ratio (width/height of the output image)\n",
        "        near_plane = 0.1  # Near clipping plane distance\n",
        "        far_plane = 100  # Far clipping plane distance\n",
        "\n",
        "        projection_matrix = p.computeProjectionMatrixFOV(fov, aspect_ratio, near_plane, far_plane)\n",
        "\n",
        "        # Capture the image from the camera's viewpoint\n",
        "        width = 480\n",
        "        height = 270\n",
        "        image = p.getCameraImage(width, height, view_matrix, projection_matrix)\n",
        "\n",
        "        return np.array(image[2])  # Return the RGB image (height x width x 4)\n",
        "\n",
        "    def convInttoBase3(self, x: int):\n",
        "        out = [0,0,0,0,0,0]\n",
        "        if x > 0 and x < 730:\n",
        "            j = 5\n",
        "            for i in range(6):\n",
        "                out[j] = x % 3\n",
        "                x = x // 3\n",
        "                j -= 1\n",
        "        return out\n",
        "\n",
        "    def setEndPoint(self):\n",
        "        self.endPoint, _ = p.getBasePositionAndOrientation(self.TrayId)\n",
        "        self.endPoint[2] += 0.2\n",
        "\n",
        "    def __del__(self):\n",
        "        # ensure that simulation is ended on death of object\n",
        "        if self.physics_client is not None:\n",
        "            p.disconnect()\n",
        "\n",
        "# initialize instance of simulation\n",
        "env = ArmEnv(False)"
      ],
      "metadata": {
        "id": "T4TVsWq7pdPT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This next block simply ensures that pybullet is connected correctly."
      ],
      "metadata": {
        "id": "mz-qr2bOEukI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#testing will be removed on final implementation\n",
        "env.connect()\n",
        "env.load_environment()\n",
        "\n",
        "env.resetSim()\n",
        "\n",
        "obj = []\n",
        "\n",
        "for i in range(50):\n",
        "\n",
        "    observation, reward, terminated = env.takeAction(360)\n",
        "\n",
        "    #print(f\"observation shape: {observation[0].shape}, {observation[1].shape}\")\n",
        "    #print(f\"reward: {reward}\")\n",
        "    #print(f\"terminated: {terminated}\")\n",
        "\n",
        "    obj = observation[0]\n",
        "\n",
        "    time.sleep(1./240.)\n",
        "\n",
        "# Display the image using Matplotlib\n",
        "plt.imshow(obj)\n",
        "plt.axis('off')  # Hide axes for better visualization\n",
        "plt.show()\n",
        "\n",
        "env.disconnect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "dFR2Fv3rpjck",
        "outputId": "6b739c58-c549-4a3c-a6be-1c3e4d1a4310"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "connected to pybullet simulation\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "diconnected from pybullet simulation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Replay Memory\n",
        "\n",
        "Next we define an object to hold the replay memory as well as a class to hold each memory and keep it to a certain size."
      ],
      "metadata": {
        "id": "eBzC_-AtE7gu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the transition model/states\n",
        "Transition = namedtuple('Transition', ('state1', 'state2', 'action', 'next_state1', 'next_state2', 'reward'))\n",
        "\n",
        "# class to hold the replay memory\n",
        "class ReplayMemory(object):\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = deque([], maxlen=capacity)\n",
        "\n",
        "    def push(self, *args):\n",
        "        \"\"\"Save a transition\"\"\"\n",
        "        self.memory.append(Transition(*args))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ],
      "metadata": {
        "id": "iUmRi2LQtbBd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the Network Architecture\n",
        "\n",
        "This is where we define the actual shape of the network we are going to use from the number of layers to how each of those layers behave.\n",
        "\n",
        "Our network is composed of 7 layers with the first 3 being convolutional and the last 4 being fully connected.  The first 3 handle the camera data to find usefull features and help shrink it down to be handleable by the fully connected layers.  When the camera data is flattened it is also concatenated with some other data, the servo angles, then it is passed through 4 fully connected layers to eventually have an output of 729 units."
      ],
      "metadata": {
        "id": "E_HmRegGFI-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a class for the nueral network model\n",
        "class ArmDQN(nn.Module):\n",
        "\n",
        "    def __init__(self, n_actions):\n",
        "        super(ArmDQN, self).__init__()\n",
        "        # first layer goes from 3 input channels to 6 output channels\n",
        "        self.layer1 = nn.Conv2d(3, 3, kernel_size=3, padding=1)\n",
        "        # second layer has 6 input channels and goes to 3 output channels\n",
        "        self.layer2 = nn.Conv2d(3, 3, kernel_size=3, padding=1)\n",
        "        # second layer has 6 input channels and goes to 3 output channels\n",
        "        self.layer3 = nn.Conv2d(3, 3, kernel_size=3, padding=1)\n",
        "        # layer for switching to fully connected linear network\n",
        "        # is set to none since input must be set dynamically based on the convolutional network\n",
        "        # the input amount is correct for initial input of size [ 3, 480, 270]\n",
        "        self.fc1 = nn.Linear(5947, 5832)\n",
        "        # second linear layer\n",
        "        self.fc2 = nn.Linear(5832, 2916)\n",
        "        # third linear layer\n",
        "        self.fc3 = nn.Linear(2916, 1458)\n",
        "        # fourth linear layer\n",
        "        self.fc4 = nn.Linear(1458, n_actions)\n",
        "\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        x = self.layer1(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = F.relu(x)\n",
        "        x = self.layer2(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = F.relu(x)\n",
        "        x = self.layer3(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = F.relu(x)\n",
        "        x = x.flatten(1)\n",
        "        x = torch.cat((x, y), dim=1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc4(x)\n",
        "        # x = F.relu(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "D3Lwotf1tsXY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining Hyperparameters\n",
        "\n",
        "Next we define our hyperparameters for the model\n",
        "\n",
        "\n",
        "*   Batch_Size - defines how large of a batch to use when optimizing the policy\n",
        "*   Gamma - determines the weight of time on the aproximation of values\n",
        "*   Epsilon - this determines how much the model should explore vs. exploit.  It is composed of 3 parts the start the end and the decay the start and end define the limits and the decay defines how many steps it takes to get to the end.\n",
        "*   Tau - controls the influence of the policy_network vs target_network weights on updating the target_network\n",
        "*   Learning Rate - controls how much to update the weights of the policy network when doing backpropagation\n",
        "*   Memory Size - controls the capacity of the Replay memory\n",
        "*   Max Episode Duration - controls the maximum amount of steps that can be spent in one episode\n",
        "\n"
      ],
      "metadata": {
        "id": "_DxyRtkwH2oC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# setup which device to use\n",
        "device = torch.device(\n",
        "    \"cuda\" if torch.cuda.is_available() else\n",
        "    \"mps\" if torch.backends.mps.is_available() else\n",
        "    \"cpu\"\n",
        ")\n",
        "\n",
        "\n",
        "# BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
        "# GAMMA is the discount factor as mentioned in the previous section\n",
        "# EPS_START is the starting value of epsilon\n",
        "# EPS_END is the final value of epsilon\n",
        "# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
        "# TAU is the update rate of the target network\n",
        "# LR is the learning rate of the ``AdamW`` optimizer\n",
        "BATCH_SIZE = 128\n",
        "GAMMA = 0.99\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 10000\n",
        "TAU = 0.005\n",
        "LR = 1e-5\n",
        "MEMORYSIZE = 4000\n",
        "MAXEPISODEDURATION = 100\n",
        "\n",
        "if torch.cuda.is_available() or torch.backends.mps.is_available():\n",
        "    num_episodes = 100\n",
        "else:\n",
        "    num_episodes = 1"
      ],
      "metadata": {
        "id": "xpJw4-qAtuZK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize the Networks\n",
        "\n",
        "Next we setup variables to hold data we use to record the results of training.\n",
        "\n",
        "* episode_durations - how long on each episode was spent\n",
        "* total_rewards - what was the total cumulative reward for an episode\n",
        "* max_phase - what was the maximum phase reached in an episode\n",
        "* end_phase - what was the phase the episode ended on\n",
        "* time_per_phase - how much time of the episode was spent in each phase\n",
        "\n",
        "This is also where both the policy and the target networks are initialized and there initial states set equal to each other."
      ],
      "metadata": {
        "id": "psWQThlXNxmX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# number of actions\n",
        "n_actions = env.actionSpace\n",
        "\n",
        "# variables for recording\n",
        "# list of episode lengths\n",
        "episode_durations = []\n",
        "# list of total rewards\n",
        "total_rewards = []\n",
        "# list of max phase\n",
        "max_phase = []\n",
        "end_phase = []\n",
        "# list of time per phase\n",
        "time_per_phase = []\n",
        "\n",
        "policy_net = ArmDQN(n_actions).to(device)\n",
        "target_net = ArmDQN(n_actions).to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
        "memory = ReplayMemory(MEMORYSIZE)\n",
        "\n",
        "steps_done = [0, 0, 0, 0]\n"
      ],
      "metadata": {
        "id": "mmtR-Q4wt6MT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Select Action Function\n",
        "\n",
        "This function selectes the next action to take with epsilon greedy algorithm.\n",
        "This allows the model to explore by taking random steps and eventually following the model more and more."
      ],
      "metadata": {
        "id": "2ytQ6e_UtoMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# select which action to take next\n",
        "# either us the policy_net output or get rand action\n",
        "def select_action(state1, state2, phase):\n",
        "    global steps_done\n",
        "    sample = random.random()\n",
        "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
        "        math.exp(-1. * steps_done[phase]/ EPS_DECAY)\n",
        "    steps_done[phase] += 1\n",
        "\n",
        "    if sample > eps_threshold:\n",
        "        with torch.no_grad():\n",
        "            return policy_net(state1, state2).max(1).indices.view(1, 1)\n",
        "    else:\n",
        "        return torch.tensor([[random.randint(0, (n_actions - 1))]], device=device, dtype=torch.long)"
      ],
      "metadata": {
        "id": "Fd0ZeKctuJAb"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimize Model Function\n",
        "\n",
        "This function is where the policy network is optimized.  A batch is called from the replay memory and used to updat the policy network using results from the target network which is soft updated at each step."
      ],
      "metadata": {
        "id": "kcDBra4gt74F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def optimize_model():\n",
        "    # check if enouch action state pairs in memory\n",
        "    if len(memory) < BATCH_SIZE:\n",
        "        return\n",
        "\n",
        "    # get a sample from memory of batch size\n",
        "    transitions = memory.sample(BATCH_SIZE)\n",
        "\n",
        "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
        "    # detailed explanation). This converts batch-array of Transitions\n",
        "    # to Transition of batch-arrays.\n",
        "    batch = Transition(*zip(*transitions))\n",
        "\n",
        "    # Compute a mask of non-final states and concatenate the batch elements\n",
        "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state1)), device=device, dtype=torch.bool)\n",
        "    non_final_next_states1 = torch.cat([s for s in batch.next_state1 if s is not None])\n",
        "    non_final_next_states2 = torch.cat([s for s in batch.next_state2 if s is not None])\n",
        "    state1_batch = torch.cat(batch.state1)\n",
        "    state2_batch = torch.cat(batch.state2)\n",
        "    action_batch = torch.cat(batch.action)\n",
        "    reward_batch = torch.cat(batch.reward)\n",
        "\n",
        "    # Compute Q(s_t, a) with the model\n",
        "    # these are the actions which would've been taken for each batch state according to policy\n",
        "    state_action_values = policy_net(state1_batch, state2_batch).gather(1, action_batch)\n",
        "\n",
        "    # Compute V(s_{t+1}) for all next states.\n",
        "    # Expected values of actions for non_final_next_states are computed based\n",
        "    # on the \"older\" target_net; selecting their best reward with max(1).values\n",
        "    # This is merged based on the mask, such that we'll have either the expected\n",
        "    # state value or 0 in case the state was final.\n",
        "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        next_state_values[non_final_mask] = target_net(non_final_next_states1, non_final_next_states2).max(1).values\n",
        "\n",
        "    #Compute the expected Q values\n",
        "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
        "\n",
        "    # Compute Hubber loss\n",
        "    criterion = nn.SmoothL1Loss()\n",
        "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "\n",
        "    # Optimize the model\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    # In-place gradient clipping\n",
        "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "6GrbRiN_3dMj"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run Model\n",
        "\n",
        "In this last step the model is run taking actions in the simulated environment and getting fead back to train with based on the results."
      ],
      "metadata": {
        "id": "vww2aj3GuMzo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env.connect()\n",
        "# env.load_environment()\n",
        "\n",
        "for i_episodes in range(num_episodes):\n",
        "    # append total reward with 0 and add two it each step of the episode\n",
        "    total_rewards.append(0)\n",
        "    time_per_phase.append([0,0,0,0])\n",
        "    max_phase.append(0)\n",
        "\n",
        "    state1, state2 = env.resetSim()\n",
        "    state1 = torch.tensor(state1, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "    state1 = state1.permute(0, 3, 2, 1)\n",
        "    state2 = torch.tensor(state2, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "\n",
        "    print(f\"Episode: {i_episodes}\")\n",
        "\n",
        "    for t in count():\n",
        "        prev_phase = env.phase\n",
        "\n",
        "        action = select_action(state1, state2, env.phase)\n",
        "        observation, reward, terminated = env.takeAction(action.item())\n",
        "        total_rewards[i_episodes] += reward\n",
        "        reward = torch.tensor([reward], device=device)\n",
        "\n",
        "        if terminated:\n",
        "          next_state1 = None\n",
        "          next_state2 = None\n",
        "        else:\n",
        "          next_state1 = torch.tensor(observation[0], dtype=torch.float32, device=device).unsqueeze(0)\n",
        "          next_state1 = next_state1.permute(0, 3, 2, 1)\n",
        "          next_state2 = torch.tensor(observation[1], dtype=torch.float32, device=device).unsqueeze(0)\n",
        "\n",
        "        # store transition in memory\n",
        "        memory.push(state1, state2, action, next_state1, next_state2, reward)\n",
        "\n",
        "        # move to next state\n",
        "        state1 = next_state1\n",
        "        state2 = next_state2\n",
        "\n",
        "        # optimize the model\n",
        "        optimize_model()\n",
        "\n",
        "        # Soft update of the target network's weights\n",
        "        # θ′ ← τ θ + (1 −τ )θ′\n",
        "        target_net_state_dict = target_net.state_dict()\n",
        "        policy_net_state_dict = policy_net.state_dict()\n",
        "        for key in policy_net_state_dict:\n",
        "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
        "        target_net.load_state_dict(target_net_state_dict)\n",
        "\n",
        "        if prev_phase != env.phase:\n",
        "            time_per_phase[i_episodes][prev_phase] += t + 1\n",
        "\n",
        "        if prev_phase < env.phase:\n",
        "            max_phase[i_episodes] = env.phase\n",
        "\n",
        "        if terminated or t > MAXEPISODEDURATION:\n",
        "            episode_durations.append(t + 1)\n",
        "            end_phase.append(env.phase)\n",
        "            time_per_phase[i_episodes][env.phase] = t + 1\n",
        "\n",
        "            print(f\"Total Reward: {total_rewards[i_episodes]}\")\n",
        "            print(f\"Episode Duration: {t + 1}\")\n",
        "            print(f\"Max Phase: {max_phase[i_episodes]}\")\n",
        "            print(f\"End Phase: {end_phase[i_episodes]}\")\n",
        "            print(f\"Time Per Phase: {time_per_phase[i_episodes]}\")\n",
        "            print(\"\")\n",
        "\n",
        "            break\n",
        "\n",
        "\n",
        "year = datetime.today()\n",
        "time = datetime.now().time()\n",
        "testId = \"NoNegativeReward\"\n",
        "\n",
        "formatted_year = year.strftime(\"%Y-%m-%d\")\n",
        "formatted_time = time.strftime(\"%H:%M:%S\")\n",
        "\n",
        "directory = \"/content/drive/MyDrive/166_Project/Models/\" + formatted_year\n",
        "if not os.path.exists(directory):\n",
        "    os.mkdir(directory)\n",
        "\n",
        "model_path = directory + \"/\" + testId + \"_\" + formatted_time + \".pth\"\n",
        "\n",
        "torch.save(policy_net.state_dict(), model_path)\n",
        "\n",
        "directory = \"/content/drive/MyDrive/166_Project/Results/\" + formatted_year\n",
        "if not os.path.exists(directory):\n",
        "    os.mkdir(directory)\n",
        "\n",
        "testInfo_path = directory + \"/\" + testId + \"_\" + formatted_time + \".csv\"\n",
        "\n",
        "with open(testInfo_path, \"w\") as file:\n",
        "    file.write(\"Test Information for \" + testId)\n",
        "\n",
        "with open(testInfo_path, \"a\") as file:\n",
        "    file.write(\"\\nnum_episodes, batch_size, Gamma, Eps_start, Eps_end, Eps_decay, Tau, Learning_rate, memory_size, max_episode_duration\")\n",
        "    file.write(f\"\\n{num_episodes}, {BATCH_SIZE}, {GAMMA}, {EPS_START}, {EPS_END}, {EPS_DECAY}, {TAU}, {LR}, {MEMORYSIZE}, {MAXEPISODEDURATION}\")\n",
        "    file.write(\"\\nepisode, total_rewards, duration, max_phase, end_phase, time_in_phase1, time_in_phase2, time_in_phase3, time_in_phase4\")\n",
        "    for i in range(num_episodes):\n",
        "        file.write(f\"\\n{i}, {total_rewards[i]}, {episode_durations[i]}, {max_phase[i]}, {end_phase[i]}, {time_per_phase[i][0]}, {time_per_phase[i][1]}, {time_per_phase[i][2]}, {time_per_phase[i][3]}\")\n",
        "\n",
        "env.disconnect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43bxyUTT3onq",
        "outputId": "9f497380-c6be-4ab1-cd06-072065d274c0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "connected to pybullet simulation\n",
            "Episode: 0\n",
            "Total Reward: -5\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 1\n",
            "Total Reward: -43\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 2\n",
            "Total Reward: -25\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 3\n",
            "Total Reward: 11\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 4\n",
            "Total Reward: -80\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 5\n",
            "Total Reward: -18\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 6\n",
            "Total Reward: -93\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 7\n",
            "Total Reward: -8\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 8\n",
            "Total Reward: -31\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 9\n",
            "Total Reward: 17\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 10\n",
            "Total Reward: 40\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 11\n",
            "Total Reward: -19\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 12\n",
            "Total Reward: 12\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 13\n",
            "Total Reward: 24\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 14\n",
            "Total Reward: 54\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 15\n",
            "Total Reward: 80\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 16\n",
            "Total Reward: 35\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 17\n",
            "Total Reward: 45\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 18\n",
            "Total Reward: 33\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 19\n",
            "Total Reward: 41\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 20\n",
            "Total Reward: 7\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 21\n",
            "Total Reward: -28\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 22\n",
            "Total Reward: 50\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 23\n",
            "Total Reward: 55\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 24\n",
            "Total Reward: 17\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 25\n",
            "Total Reward: 57\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 26\n",
            "Total Reward: 64\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 27\n",
            "Total Reward: 73\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 28\n",
            "Total Reward: 27\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 29\n",
            "Total Reward: 61\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 30\n",
            "Total Reward: 47\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 31\n",
            "Total Reward: 69\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 32\n",
            "Total Reward: -55\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 33\n",
            "Total Reward: -129\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 34\n",
            "Total Reward: 72\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 35\n",
            "Total Reward: 53\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 36\n",
            "Total Reward: 78\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 37\n",
            "Total Reward: -87\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 38\n",
            "Total Reward: 88\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 39\n",
            "Total Reward: 93\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 40\n",
            "Total Reward: 75\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 41\n",
            "Total Reward: 73\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 42\n",
            "Total Reward: 108\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 43\n",
            "Total Reward: 111\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 44\n",
            "Total Reward: 74\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 45\n",
            "Total Reward: 70\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 46\n",
            "Total Reward: 50\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 47\n",
            "Total Reward: 90\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 48\n",
            "Total Reward: 38\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 49\n",
            "Total Reward: 100\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 50\n",
            "Total Reward: -10\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 51\n",
            "Total Reward: 24\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 52\n",
            "Total Reward: 52\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 53\n",
            "Total Reward: 54\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 54\n",
            "Total Reward: 85\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 55\n",
            "Total Reward: 71\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 56\n",
            "Total Reward: 27\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 57\n",
            "Total Reward: 19\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 58\n",
            "Total Reward: 102\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 59\n",
            "Total Reward: 80\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 60\n",
            "Total Reward: -8\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 61\n",
            "Total Reward: 88\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 62\n",
            "Total Reward: 78\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 63\n",
            "Total Reward: 91\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 64\n",
            "Total Reward: 87\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 65\n",
            "Total Reward: 57\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 66\n",
            "Total Reward: -23\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 67\n",
            "Total Reward: 142\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 68\n",
            "Total Reward: -53\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 69\n",
            "Total Reward: 50\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 70\n",
            "Total Reward: 55\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 71\n",
            "Total Reward: 70\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 72\n",
            "Total Reward: 114\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 73\n",
            "Total Reward: 23\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 74\n",
            "Total Reward: 93\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 75\n",
            "Total Reward: 86\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 76\n",
            "Total Reward: 116\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 77\n",
            "Total Reward: 20\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 78\n",
            "Total Reward: 130\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 79\n",
            "Total Reward: 58\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 80\n",
            "Total Reward: 3\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 81\n",
            "Total Reward: 131\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 82\n",
            "Total Reward: 114\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 83\n",
            "Total Reward: -47\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 84\n",
            "Total Reward: 138\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 85\n",
            "Total Reward: 107\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 86\n",
            "Total Reward: -11\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 87\n",
            "Total Reward: 103\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 88\n",
            "Total Reward: 116\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 89\n",
            "Total Reward: 66\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 90\n",
            "Total Reward: 107\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 91\n",
            "Total Reward: 62\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 92\n",
            "Total Reward: 71\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 93\n",
            "Total Reward: 115\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 94\n",
            "Total Reward: 96\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 95\n",
            "Total Reward: -11\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 96\n",
            "Total Reward: 153\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 97\n",
            "Total Reward: 115\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 98\n",
            "Total Reward: 103\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 99\n",
            "Total Reward: 50\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 100\n",
            "Total Reward: 154\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 101\n",
            "Total Reward: 141\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 102\n",
            "Total Reward: 115\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 103\n",
            "Total Reward: 44\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 104\n",
            "Total Reward: -2\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 105\n",
            "Total Reward: 103\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 106\n",
            "Total Reward: -175\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 107\n",
            "Total Reward: 121\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 108\n",
            "Total Reward: 90\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 109\n",
            "Total Reward: -64\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 110\n",
            "Total Reward: 100\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 111\n",
            "Total Reward: 118\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 112\n",
            "Total Reward: 67\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 113\n",
            "Total Reward: 53\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 114\n",
            "Total Reward: 82\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 115\n",
            "Total Reward: 138\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 116\n",
            "Total Reward: 43\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 117\n",
            "Total Reward: 51\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 118\n",
            "Total Reward: 141\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 119\n",
            "Total Reward: 143\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 120\n",
            "Total Reward: -12\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 121\n",
            "Total Reward: 69\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 122\n",
            "Total Reward: 131\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 123\n",
            "Total Reward: 116\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 124\n",
            "Total Reward: 91\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 125\n",
            "Total Reward: 129\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 126\n",
            "Total Reward: 140\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 127\n",
            "Total Reward: 124\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 128\n",
            "Total Reward: 46\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 129\n",
            "Total Reward: 38\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 130\n",
            "Total Reward: 125\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 131\n",
            "Total Reward: 127\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 132\n",
            "Total Reward: 132\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 133\n",
            "Total Reward: 32\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 134\n",
            "Total Reward: 124\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 135\n",
            "Total Reward: -73\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 136\n",
            "Total Reward: 95\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 137\n",
            "Total Reward: 61\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 138\n",
            "Total Reward: 116\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 139\n",
            "Total Reward: 108\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 140\n",
            "Total Reward: 138\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 141\n",
            "Total Reward: 126\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 142\n",
            "Total Reward: 61\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 143\n",
            "Total Reward: 70\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 144\n",
            "Total Reward: 114\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 145\n",
            "Total Reward: -73\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 146\n",
            "Total Reward: -72\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 147\n",
            "Total Reward: 110\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 148\n",
            "Total Reward: 127\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 149\n",
            "Total Reward: 89\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 150\n",
            "Total Reward: 159\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 151\n",
            "Total Reward: 122\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 152\n",
            "Total Reward: 111\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 153\n",
            "Total Reward: 149\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 154\n",
            "Total Reward: 133\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 155\n",
            "Total Reward: 159\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 156\n",
            "Total Reward: 39\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 157\n",
            "Total Reward: 64\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 158\n",
            "Total Reward: 150\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 159\n",
            "Total Reward: 131\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 160\n",
            "Total Reward: -11\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 161\n",
            "Total Reward: 138\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 162\n",
            "Total Reward: 164\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 163\n",
            "Total Reward: 76\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 164\n",
            "Total Reward: 104\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 165\n",
            "Total Reward: 66\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 166\n",
            "Total Reward: 108\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 167\n",
            "Total Reward: 11\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 168\n",
            "Total Reward: 156\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 169\n",
            "Total Reward: 67\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 170\n",
            "Total Reward: 49\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 171\n",
            "Total Reward: 24\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 172\n",
            "Total Reward: 171\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 173\n",
            "Total Reward: 136\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 174\n",
            "Total Reward: 143\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 175\n",
            "Total Reward: 138\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 176\n",
            "Total Reward: 83\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 177\n",
            "Total Reward: 148\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 178\n",
            "Total Reward: 166\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 179\n",
            "Total Reward: 122\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 180\n",
            "Total Reward: -208\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 181\n",
            "Total Reward: -3\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 182\n",
            "Total Reward: 49\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 183\n",
            "Total Reward: 146\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 184\n",
            "Total Reward: -6\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 185\n",
            "Total Reward: 97\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 186\n",
            "Total Reward: -244\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 187\n",
            "Total Reward: 121\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 188\n",
            "Total Reward: 150\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 189\n",
            "Total Reward: 83\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 190\n",
            "Total Reward: 114\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 191\n",
            "Total Reward: -82\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 192\n",
            "Total Reward: 68\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 193\n",
            "Total Reward: 52\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 194\n",
            "Total Reward: 86\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 195\n",
            "Total Reward: 124\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 196\n",
            "Total Reward: 135\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 197\n",
            "Total Reward: 26\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 198\n",
            "Total Reward: 107\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 199\n",
            "Total Reward: 26\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 200\n",
            "Total Reward: 16\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 201\n",
            "Total Reward: 45\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 202\n",
            "Total Reward: 24\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 203\n",
            "Total Reward: 178\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 204\n",
            "Total Reward: -11\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 205\n",
            "Total Reward: 69\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 206\n",
            "Total Reward: 66\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 207\n",
            "Total Reward: 145\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 208\n",
            "Total Reward: 101\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 209\n",
            "Total Reward: 66\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 210\n",
            "Total Reward: 154\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 211\n",
            "Total Reward: 50\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 212\n",
            "Total Reward: -224\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 213\n",
            "Total Reward: 137\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 214\n",
            "Total Reward: -12\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 215\n",
            "Total Reward: 155\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 216\n",
            "Total Reward: 120\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 217\n",
            "Total Reward: 142\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 218\n",
            "Total Reward: -76\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 219\n",
            "Total Reward: 72\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 220\n",
            "Total Reward: 133\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 221\n",
            "Total Reward: 150\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 222\n",
            "Total Reward: 167\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 223\n",
            "Total Reward: 98\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 224\n",
            "Total Reward: 91\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 225\n",
            "Total Reward: 86\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 226\n",
            "Total Reward: -15\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 227\n",
            "Total Reward: 101\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 228\n",
            "Total Reward: 188\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 229\n",
            "Total Reward: 149\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 230\n",
            "Total Reward: 138\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 231\n",
            "Total Reward: 149\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 232\n",
            "Total Reward: 61\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 233\n",
            "Total Reward: 11\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 234\n",
            "Total Reward: 86\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 235\n",
            "Total Reward: -15\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 236\n",
            "Total Reward: 103\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 237\n",
            "Total Reward: 134\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 238\n",
            "Total Reward: 54\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 239\n",
            "Total Reward: 172\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 240\n",
            "Total Reward: 103\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 241\n",
            "Total Reward: 111\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 242\n",
            "Total Reward: 108\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 243\n",
            "Total Reward: 154\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 244\n",
            "Total Reward: 36\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 245\n",
            "Total Reward: 111\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 246\n",
            "Total Reward: 43\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 247\n",
            "Total Reward: 126\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 248\n",
            "Total Reward: 50\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "Episode: 249\n",
            "Total Reward: 108\n",
            "Episode Duration: 102\n",
            "Max Phase: 0\n",
            "End Phase: 0\n",
            "Time Per Phase: [102, 0, 0, 0]\n",
            "\n",
            "diconnected from pybullet simulation\n"
          ]
        }
      ]
    }
  ]
}